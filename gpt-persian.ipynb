{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1427869,"sourceType":"datasetVersion","datasetId":836206}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-09T21:14:57.549219Z","iopub.execute_input":"2024-06-09T21:14:57.550524Z","iopub.status.idle":"2024-06-09T21:15:02.096960Z","shell.execute_reply.started":"2024-06-09T21:14:57.550467Z","shell.execute_reply":"2024-06-09T21:15:02.096082Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\n\nfile_paths = [\n    \"/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-1.txt\",\n#     \"/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-2.txt\",\n#     \"/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-3.txt\",\n#     \"/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-4.txt\",\n#     \"/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-5.txt\",\n#     \"/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-6.txt\",\n#     \"/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-7.txt\",\n#     \"/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-8.txt\",\n#     \"/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-9.txt\"\n]\n\ntext = \"\"\n\nfor file_path in file_paths:\n    with open(file_path, 'r', encoding='utf-8') as file:\n        text += file.read()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:16:30.393550Z","iopub.execute_input":"2024-06-09T21:16:30.393950Z","iopub.status.idle":"2024-06-09T21:16:31.469464Z","shell.execute_reply.started":"2024-06-09T21:16:30.393919Z","shell.execute_reply":"2024-06-09T21:16:31.468368Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"lines = text.split('\\n')\n\nlen(lines)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T17:45:31.663155Z","iopub.execute_input":"2024-06-09T17:45:31.663451Z","iopub.status.idle":"2024-06-09T17:45:33.224974Z","shell.execute_reply.started":"2024-06-09T17:45:31.663428Z","shell.execute_reply":"2024-06-09T17:45:33.223971Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"7547845"},"metadata":{}}]},{"cell_type":"code","source":"len(text)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T17:45:33.227864Z","iopub.execute_input":"2024-06-09T17:45:33.228482Z","iopub.status.idle":"2024-06-09T17:45:33.235324Z","shell.execute_reply.started":"2024-06-09T17:45:33.228442Z","shell.execute_reply":"2024-06-09T17:45:33.234509Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"478110829"},"metadata":{}}]},{"cell_type":"code","source":"!pip install hazm\n!pip install num2fawords\n!pip install parsinorm","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-09T21:16:34.504665Z","iopub.execute_input":"2024-06-09T21:16:34.505652Z","iopub.status.idle":"2024-06-09T21:17:11.529590Z","shell.execute_reply.started":"2024-06-09T21:16:34.505596Z","shell.execute_reply":"2024-06-09T21:17:11.528384Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: hazm in /opt/conda/lib/python3.10/site-packages (0.7.0)\nRequirement already satisfied: nltk==3.3 in /opt/conda/lib/python3.10/site-packages (from hazm) (3.3)\nRequirement already satisfied: libwapiti>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from hazm) (0.2.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk==3.3->hazm) (1.16.0)\nRequirement already satisfied: num2fawords in /opt/conda/lib/python3.10/site-packages (1.1)\nRequirement already satisfied: parsinorm in /opt/conda/lib/python3.10/site-packages (0.0.3)\nRequirement already satisfied: num2fawords==1.1 in /opt/conda/lib/python3.10/site-packages (from parsinorm) (1.1)\nRequirement already satisfied: persian-tools==0.0.10 in /opt/conda/lib/python3.10/site-packages (from parsinorm) (0.0.10)\nRequirement already satisfied: urlextract==1.4.0 in /opt/conda/lib/python3.10/site-packages (from parsinorm) (1.4.0)\nRequirement already satisfied: nltk==3.3 in /opt/conda/lib/python3.10/site-packages (from parsinorm) (3.3)\nRequirement already satisfied: hazm==0.7.0 in /opt/conda/lib/python3.10/site-packages (from parsinorm) (0.7.0)\nRequirement already satisfied: libwapiti>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from hazm==0.7.0->parsinorm) (0.2.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk==3.3->parsinorm) (1.16.0)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from urlextract==1.4.0->parsinorm) (3.6)\nRequirement already satisfied: uritools in /opt/conda/lib/python3.10/site-packages (from urlextract==1.4.0->parsinorm) (4.0.3)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from urlextract==1.4.0->parsinorm) (1.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from urlextract==1.4.0->parsinorm) (3.13.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nfrom hazm import Normalizer, InformalNormalizer, word_tokenize\nfrom num2fawords import words\nfrom parsinorm import General_normalization, Date_time_to_text, Abbreviation\nfrom string import punctuation\n\ndef normalize(text):\n    hazm = Normalizer()\n    text = hazm.normalize(text)\n#     print(\"1\", text)\n#     text = re.sub('''[!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~؟،٪×÷»«]+''', ' ', text)\n#     text = ' '.join(text.split())\n    text = text.replace('ء', '')\n    text = text.replace('أ', 'ا')\n    text = text.replace('ؤ', 'و')\n    text = text.replace('ئ', 'ی')\n    pattern = r'[^آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی \\u200C]'\n    text = re.sub(pattern, '', text)\n#     print(\"2\", text)\n    tokens = word_tokenize(text)\n    filtered_tokens = [token for token in tokens if not any(char.isalpha() and ord(char) < 128 for char in token)]\n    text = ' '.join(filtered_tokens)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:17:11.532303Z","iopub.execute_input":"2024-06-09T21:17:11.533265Z","iopub.status.idle":"2024-06-09T21:17:11.542225Z","shell.execute_reply.started":"2024-06-09T21:17:11.533218Z","shell.execute_reply":"2024-06-09T21:17:11.541283Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"sentences = text.splitlines()\n\nfor sentence in sentences[:5]:\n    print(\"1\", sentence)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:17:11.543375Z","iopub.execute_input":"2024-06-09T21:17:11.543703Z","iopub.status.idle":"2024-06-09T21:17:12.390012Z","shell.execute_reply.started":"2024-06-09T21:17:11.543669Z","shell.execute_reply":"2024-06-09T21:17:12.388981Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"1 عنوان مقاله: صفحهٔ اصلی\n1 \n1 <templatestyles src=\"صفحه اصلی/\n1 \n1 \n","output_type":"stream"}]},{"cell_type":"code","source":"len(sentences)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:20:43.481547Z","iopub.execute_input":"2024-06-08T15:20:43.481906Z","iopub.status.idle":"2024-06-08T15:20:43.488360Z","shell.execute_reply.started":"2024-06-08T15:20:43.481878Z","shell.execute_reply":"2024-06-08T15:20:43.487373Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"7547847"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\ncleaned_sentences = [sentence for sentence in tqdm(sentences) if sentence != \"\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:17:12.391982Z","iopub.execute_input":"2024-06-09T21:17:12.392330Z","iopub.status.idle":"2024-06-09T21:17:12.640477Z","shell.execute_reply.started":"2024-06-09T21:17:12.392301Z","shell.execute_reply":"2024-06-09T21:17:12.639434Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"\n  0%|          | 0/446023 [00:00<?, ?it/s]\u001b[A\n 35%|███▌      | 158189/446023 [00:00<00:00, 1581635.09it/s]\u001b[A\n100%|██████████| 446023/446023 [00:00<00:00, 1922294.70it/s]\u001b[A\n","output_type":"stream"}]},{"cell_type":"code","source":"len(cleaned_sentences)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:20:52.421547Z","iopub.execute_input":"2024-06-08T15:20:52.421877Z","iopub.status.idle":"2024-06-08T15:20:52.427876Z","shell.execute_reply.started":"2024-06-08T15:20:52.421853Z","shell.execute_reply":"2024-06-08T15:20:52.426935Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"2882884"},"metadata":{}}]},{"cell_type":"code","source":"for sentence in cleaned_sentences[:5]:\n    print(\"1\", sentence)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:20:58.074807Z","iopub.execute_input":"2024-06-08T15:20:58.075160Z","iopub.status.idle":"2024-06-08T15:20:58.079832Z","shell.execute_reply.started":"2024-06-08T15:20:58.075130Z","shell.execute_reply":"2024-06-08T15:20:58.078922Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"1 عنوان مقاله: صفحهٔ اصلی\n1 <templatestyles src=\"صفحه اصلی/\n1 عنوان مقاله: ویکی پدیا\n1 ویکی پدیا (کوته نوشت به صورت «وپ» و «WP») یک دانشنامه برخط چندزبانه مبتنی بر وب با محتوای آزاد و همکاری باز است که با همکاری افراد داوطلب نوشته می شود و هر کسی که به اینترنت و وب دسترسی داشته باشد می تواند مقالات آن را ببیند و ویرایش کند. نام ویکی پدیا واژه ای ترکیبی است که از واژه های ویکی (وبگاه مشارکتی) و اِنسایکلوپدیا (Encyclopedia) (دانشنامه یا دائرةالمعارف) گرفته شده است. هدف ویکی پدیا آفرینش و انتشار جهانی یک دانشنامه با محتوای آزاد به تمامی زبان های زندهٔ دنیا است.\n1 ویکی پدیای انگلیسی در تاریخ ۱۵ ژانویه ۲۰۰۱ (۲۶ دی ۱۳۷۹) به صورت مکملی برای دانشنامهٔ تخصصی نیوپدیا نوشته شد. بنیان گذاران آن «جیمی ویلز» و «لری سنگر» هستند. هم اکنون بنیاد غیرانتفاعی ویکی مدیا پروژهٔ ویکی پدیا را پشتیبانی می کند. میزبان های اینترنتی اصلی این وبگاه در شهر تامپای فلوریدا هستند. همچنین میزبان های اضافی دیگری هم در شهرهای آمستردام و سئول به این وبگاه یاری می رسانند.\n","output_type":"stream"}]},{"cell_type":"code","source":"chars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T16:03:56.664862Z","iopub.execute_input":"2024-06-08T16:03:56.665171Z","iopub.status.idle":"2024-06-08T16:04:22.591927Z","shell.execute_reply.started":"2024-06-08T16:03:56.665144Z","shell.execute_reply":"2024-06-08T16:04:22.590913Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\n !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¡¢£¤¥¦§¨©ª«¬­®¯°±²³´µ¶·¸¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿĀāĂăĄąĆćĈĉĊċČčĎďĐđĒēĔĕĖėĘęĚěĜĝĞğĠġĢģĤĥĦħĩĪīĭĮįİıĴĵĶķĸĺĻļĽľŁłŃńŅņŇňŋŌōŏőŒœŔŕŗŘřŚśŜŝŞşŠšţŤťŦŧŨũŪūŬŭŮůűųŷŹźŻżŽžƀƁƂƃƇƈƉƊƋƌƎƏƑƒƓƕƗƘƙƜƝƞƟƠơƢƣƤƥƩƬƭƮƯưƲƳƴƵƶƿǀǁǃǍǎǏǐǒǔǖǚǜǝǟǡǣǤǥǦǧǨǪǫǭǰǴǵǸǹǽȊȋȒȓȘșȚțȜȝȞȟȠȤȥȦȧȲȳȷȻȼɀɃɄɈɉɊɋɌɍɎɏɐɑɒɓɔɕɖɗɘəɚɛɜɟɠɡɢɣɤɥɦɨɪɫɬɯɰɱɲɳɴɵɸɹɺɻɽɾʀʁʂʃʄʈʉʊʋʌʍʎʏʐʑʒʔʕʘʙʛʝʠʰʱʲʷʹʺʻʼʽʾʿˀˁ˂˃˄ˆˇˈˉˊˌˏːˑ˓˘˚˛˝ˠˢˤˮ̵̶̷̸̡̢̧̛̖̗̘̙̜̝̞̟̠̣̤̥̦̩̪̫̬̭̮̯̰̱̲̳̹̺̻̼͇͈͉͍͎̀́̂̃̄̅̆̇̈̉̊̋̌̍̎̏̐̑̒̓̔̽̾̿͂͆͊͋͌̚ͅ͏͓͔͕͖͙͚͐͑͒͗͛ͣͤͥͦͧͨͩͪͫͬͭͮͯ͘͜͟͢͝͠͡Ͱͱ΄ΆΈΉΊΌΎΏΐΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩάέήίαβγδεζηθικλμνξοπρςστυφχψωϊόύώϐϑϒϕϖϚϛϜϝϞϟϠϡϣϥϩϬϵϷϸϺϻЀЁЂЃЄІЇЈЉЊЋЍЎЏАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяѐёђѓєіїјљњћѝўџѢѣѦѧѩѫѲѳѷ҉ҐґҒғҖҗҘҙҚқҠҡҢңҤҥҪҫҮүҰұҲҳҶҷҹҺһӊӑӓӕӖӗӘәӣӦӧӨөӮӯӰӱӳԑԱԲԳԴԵԶԷԹԺԻԼԽԾԿՀՁՂՃՄՅՆՇՈՉՊՋՌՍՎՏՑՒՓՔՕՖաբգդեզէըթժիլխծկհձղճմյնշոչպջռսվտրցւփքօֆևְֱֲֳִֵֶַָֹֻּ֗֫־ֿ׀ׁׂאבגדהוזחטיךכלםמןנסעףפץצקרשתװײ׳،ؐؓ؛؜؟ءآأؤإئابةتثجحخدذرزسشصضطظعغؽـفقكلمنهوىيًٌٍَُِّْٕٖٜٓٔٗ٘ٙٚٛ٠١٢٣٤٥٦٧٨٩٪٫٬٭ٮٰٱٲٴٵٶٷٸٹٺٻټٽپٿځڃڄڅچڇڈډڌڍڏڑړڔڕږژڙښڠڤڨکڪګڬڭگڳڵںڻڼڽھۀہۂۃۆۇۈۉۊۋیۍێۏېےۓ۔ەۖۗۘۚ۞ۡۤۥ۫ۮۯ۰۱۲۳۴۵۶۷۸۹ۻ۽ۿܐܒܓܕܖܗܘܙܚܛܝܟܠܡܢܣܤܥܦܨܩܪܫܬܹܼܰܲܳܵܶܺݎݐݔݖݚݜݝݢݨݩݴݸݾހނރބޅކއވމފދތލގސޑޓޔޕޙޝޤަާިީުޫެޭޮޯްࢣँंःअआइईउऊएऐओऔकखगघङचछजझञटठडढणतथदधनपफबभमयरलळवशषसह़ािीुूृेैॉोौ्ॐ०२३७ংঅআইউকখগঘঙচছজঝঞটডঢণতথদধনপফবভমযরলশষসহ়ািীুূেৈো্ৎৰ৳ਂਅਆਇਕਖਗਘਚਜਡਣਤਦਧਨਪਫਬਭਮਰਲਵਸਹ਼ਾਿੀੁੂੇੋ੍ੜੰੱંઅઈઉકગચછજટણતથદધનપબભમયરલવશષસહાિીુૂૃેૈો્ଂଆଓଡଦଧବଳଶସିୀୁ୍ஆஇஎகஙசஜடணதநனபமயரலளழவஷஸாிீுூெேைொோ்ంఆఉకగచజటడణతదధనపబమయరలవశషసహాిుూృెేైొ్ಂಅಕಗಚಜಟಡಣತಥದನಪಬಮಯರಲಳವಶಸಹಾಿೀುೂೆೇೈ್೯ംഅകഖജടതഥദനപബഭമയരറലളഴവശഷസാിീുെേൊോ്ർൽඋකගඝජදනපමයරලවෂසහ්ාිුกขคงจฉชซฎฐณดตถทธนบปผพฟภมยรฤลวศษสหฬอะัาำิีืุูเแโไ็่้์ກຂງຊດຕທນບພມລວສຫະັາິີົເໄ່້ໍໜ་།ཀཁགངཆཇཏཐདནཔཕབམཙཚའཡརལཤསཧཨཱིེོུྐྒྗྩྫྱྲླྷကဂငစဆညတဒဓနပဘမယရလသဟအာိီုူေဴ့း္်ျြွှაბგდევზთიკლმნოპრსტუფქღყშჩცძწჭხჯሃለላሌልሕመማምሞሠረራርሮሱስሶሸቀቅበቡባቤቦተታትነናንኛአእኩክወዋውዓዘዝዞየዪያይዮደዲዳዴድጀጃጊጋግጎጣጭጽፈፊፋፍፕᐃᐊᐤᐳᐸᑎᑐᑕᑦᑯᒃᓄᖅᚦកខចជដឋណតធនពភមយរលសហាីុួៀែោះ្ᴄᴓᴗᴠᴧᵏᵒᵛᵪᵮᵶᵻᵽᶑᶕᶲḇḊḌḍḏḐḑḕḗḠḡḤḥḪḫḭḰḱḲḳḴḵḶḷḾḿṃṅṆṇṓṘṙṚṛṢṣṬṭṮṯṱṶṷẊẋẏẐẒẓẔẖẗẚạảấầậắằẵẸẹẼẽếềễệỉịọỏỐốồỗộớờợụủứừữỰựỳỷỹἀἁἂἄἅἈἉἌἍἐἑἔἕἘἙἛἜἝἠἡἤἦἨἩἬἰἱἴἵἶἷἸἹἼὀὁὃὄὅὈὉὌὍὐὑὓὔὕὙὝὨὰὲὴὶὸὺὼᾠᾱᾶᾷ᾽᾿ῃῆῇῑῖῤῥῦῬῳῴῶῷ     ​‍‎‏‐‑‒–—―‖‘’‚“”„†‡•‣…‫‬‭‮ ‰′″‴‹›※‿⁄⁉⁡⁦⁧⁩⁪⁬⁭⁮⁯⁰⁴⁵⁻ⁿ₁₂₃₄₅₆ₓ₣₤₨₩₫€₱₹⃗⃣℃℅℉ℏℓℕ№ℚℝ℞™ℤ℧℩℮ℷ⅓⅔⅙⅚⅛⅝Ⅲ←↑→↓↔↗↘↝↠↦↩↪↵⇄⇋⇌⇐⇒⇔∀∂∃∅∆∇∈∉∊∋∎∏∑−∓∕∗∘∙√∝∞∟∠∥∦∧∨∩∪∫∴∶≀≅≈≘≜≝≠≡≤≥≧≪≫⊂⊃⊆⊇⊕⊗⊞⊢⊤⊥⊨⊾⋁⋅⋆⋊⋋⋯⌈⌉⌊⌋⌐⌘⌠⍋⍳⍺⎘⎯⏘␆␕ⒶⒸⓐⓒ─│└├┤┬▄█▒■□▪▫▲▽◂◄◆◇◊○◌●◢◤◦◽☃★☆☉☌☐☑☫☬☭☰☱☲☳☴☵☶☷☺☼♀♂♈♉♊♋♌♍♎♏♐♑♒♓♔♙♟♠♣♤♥♦♧♩♬♭♮♯♿⚤⚫⚳✅✌✔✝❤❨❩➖➗➡⟨⟩⟵⟶⦜⦝⬅⬇ⱣⱤⱧⱨⱩⱪⱫⱬⱭⱮⱱⱴⲁⲉⲐⲑⲓⲙⲛⲟⲠⲥⲧⲩⲱⴰⴱⴳⴷⴻⴽⴾⵃⵇⵉⵍⵎⵏⵓⵔⵖⵙⵛⵜⵡⵢⵣ⸨⸮　、。々〈〉《》「」『』〒〔〕〖〗〜あいうえおかがきぎくぐけげこさざしじすずせそぞただちっつてでとなにぬねのはばぱひびふぶへべほぼぽまみむめもゃやゅゆょよらりるれろわゐゑをんァアィイウェエォオカガキギクグケゲコゴサザシジスズセゼソゾタダチッツテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモャヤュユョヨラリルレロワンヴヶ・ーㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣ一丁七万丈三上下不与丑世丘丙业东両两並个中串临丸丹为主乃久义之乌乐乒乓乗乘乙九也乡书乱乳乾了争事二云互五井亚亜交亥亦产享京亭亲亳亶人什仁仇今介仏仑仓仕付仙代令以们仮仲件任份企伊伎伏伐休会伝伟传伦伯伴伶伷伽佃佐体何余佛作佳使來例侍依侠侣侬侵便俊俐俗保信俣修俱倉個倍們倒候倚倫倭倻偁偃偉健傅傑傲傳像僖僧儀儉儒優儼允元兄充先光克兌兎児兒兔党入全八公六兮共关兴兵典兼内円再冒写军农决冶冷凌凡凤凯凱凸出函刀分切刑刘则初別利到制刻前剛剝剣副割劇劉劍力功加务动助効勇勋勒動勖務勝勢匂包化北匯匹区医區十千升午半华协協南博卜占卡卤卦卧卫印却卿厓厚原厥厦厳县參友双反发取受变口古句只召可台史右号司合吉同名后吏向吔吗君吞吟否启吳吴吾呂告呐员周味命和咖咸哀哈哉員哨哲唇唐唰唵商問啟喀喊喜喝喪喫喾嗎嗑嗣嘅嘉嘎嘻器噬嚣囊四回因团団园困囲図国图國園圓圖團土圣圧在圭地圳场坂坊坎块坡坤坦坪垄型垕垣埃城域執基埼堀堂堆場塔塘塚塵墟墨壁壇壎士壬壮壯売壺変复夏夔夕外多夜夢大天太夫夬央失头夷奄奇奈奎契奔奥奧女奴奶好如妃妄妈妓妖妙妥妹妻始姑姒姓委姚姜姤姥姫姬威娃娄娑娘娜娥娲娼婆婦媧媽嫖嫘嫡嫦嬉孀孁子孔字存孙孚孜孝孟季孤学孩孫學孺宁守安宋完宏宗官定宛宜宝实実客宣室宪宫宮宴家容宿寂寄寅密寇富寛察寥實寧寨寶寺寻导寿封専射将尉尊尋對小少尔尕尘尚尤尧尹尼尾局居屆屉届屋屍屏展屠履屯山岡岩岳岸峡峨峰島峽崇崎崔崴嵐嵩嵯巌巍川州巢巣工左巨巩己巳巴巷巻巽市布师希帘帝師帳常幕幡幣干平年幸幹幻幽幾广庁広庄庆庇序库应店庚府度座庫庭庵康庸廉廊廣廩廳延廷建开弁弋式弐弓弘弟张弥弦弩弱張強强弾录形彥彦彩彭影彷役彼往征径待律後徐得御徨復微徳徵德徽心忌忍志応忠忧快念怕思急性怨怪总恆恋恒恭恵悲惠惣想愁意愚愛感慈態慕慶憲憶懿戊戌戍成我戔或战戦戰戴户戸房所扃扈手才扎払托承技抄投报押抽拉拍拓括拭拳拼拾持指挚振挺捉捨掌探提換握揭損摂摩摯撃撒撫播擂操支改攻放政敎敏敗教敛散敦敬数整數文斎斐斗斜斧斯新方於旅旋族旗无既日旦早旬旭时昂昆昇昉昊昌明易星映春昭是昶時晃晉晋晓晖晟普景晴智暁暗暦暮暴曆曉曜更書曹曾替最會月有服朗望朝期木未末本札术朱朴朵机杉李材村杜束条来杨杭杯杰東杼松板极林枚果枝枢枯柄柔查柯柳柴査柾标树栗校株核根格桀桂桃案桐桑桓桜桥桶梁梅條梨棋棍棒棕棘棚棣森椒検椿楊楚楠業極楽榮槍槐様樂樊樘標模樣権横樹樺橋橘橙機檔檜檢櫛權次欢欣歌正武歩歸殤段殷殺毅母每毓比毕毗毘毛氏民气気氣水永汇汉汕汗汝江池汤決汽汾沃沈沌沖沙沢沧河油治沽泄泉法泠波泥泰泸泽洋洒洗洛洞津洪洲洵活流浄浅济浙浜浦浩浪海涅消涛涿深淳淵混清渋渓渙減渠渡渤渥温港湊湖湘湛湧湯湾湿満源準溥溪滕滝满滸滿漂演漠漢漣漫漱漸潤潮澄澤澳濟濤濱瀬灌灣火灯灵灸災炆炎炤点為烈烏烟热無然焼煎照熊熙熜熱熹熾燃燈燐燕營燧父爸爾片版牙牛牡牧物特犁犬犯状狂狐狗独狸狼猫猴猶猿獅獢獣獨獻玄玉王玛现珉珍珠班現球理琉琦琳瑛瑞瑪瑶璋璠環甄甘甚生用甫田由甲申电男町画界留畜畢略畫異畿疆疏疑病症癸発登發白百的皇皐盂盆益盖盘盛盟監盤目直相省眉県眞真眠眼着督睦睽瞻瞿矢知短石矿砂研砕破砻碁碓碩磁磐磯示礼社祁祐祖祗祚祝神祥票祭祯禁禄禅禎福禕禧禮禹离秀秋科秒秘租秦称移程種稲稽稿穂穆穌積究穷穹空突立站竜章童端競竹笑第筆等筒答策筵筷箏算管箭箱箸節篇篤簡簾籌籐米粉粒粟精糕糖糸系紀約紅納純紗紙級素索紫細紱紹組経結絕統絵絹綃綏經継続綠綬維綱綺綾総緑線編緩縄縛縣總績繆繇織繼纂續红约级纪纳纵纸绂细绍经绚统绣绥继维绶缪网罗罪置罵罷羅羊美羡群義羲羹羽羿翁翹耀老者耶耽耿联聖聚聞聡聿肉肖肝股肥肩肯育肸胡胤能脈脉脱腆腐腑腥腦腻臟臣臥臨自臭至致臺興舉舊舒舛舜舞舟航船艮良色艳艸艹艺艾节芒芙芝芬芭芰花芳芸芽苏苑苔苗苞若苦英茂范茫茶草荒荷荼莊莎莲莽菊菌菩華菱菲菻萃萌萨萬落葉葛董葵蒙蒲蒼蒿蓆蓉蓋蓝蔗蔡蔵蕉蕎薄薛薩藍藏藝藤藩蘇蘭虎虔虞號虫虾蚩蛇蛙蛛蛟蛭蛮蛯蜘融蟇蟜蟲蠱血行術街衛衡袁袋袴装裏裕製褚褪襄西要見親観覽觀观角解言計訊訓記訟詡詢試詩話語說説読誰課調談論諡諸謙謚謨證識議護變计讫讯记讷论设诀词诗话语说谓谟谩谷豆豊豌豐豚象豪豫貞財責貴費賀賁賞賢質贛贝贞贵费赛赣赤赫走赵起超越趙足跆路跳踊蹇蹴身車軍転軸軾載輔輿车轩轮软轼辅辕辛辞辨辰農边辺込辽迅运近返进远连迦迪述迷迹追送逃逆通速造逢連進逸逾遇遊運遍過道達遙遠遭遯遲選遼邊邑邓那邦邪邯邹郎郡部郭都鄂鄒鄧鄭鄱鄲酉酌酒醍醐醒里重野金針鈴鈺鉄銀銅錄錆錐錕錢錦錯録鍋鍾鎌鎧鎮鏡鐘鑑钓钟钩错锡锦镇镜長长門開閑間閔関閥閩閲閻闇闘關门问闲间闽阀阔阪阳阴阶阻阿际陆陈降限陟院陣除陰陳陵陶陸険陽隆隊随隔際隞隠隨隱难雁雄雅集雍雕雙雛雞離難雨雪雯雲雷電需震霊霜霞靈靑青靖静靡面革鞆鞍鞏韋韓韦韩音韻響順須頓領頤頬頭頼顕顗類顧顯顶顺顼顾顿颂颛额風风飛飞食飯飲飾餅養餘館饌饮饵饼首香馬駅駐駒駿験騫马骂骞骨高髙髪鬘鬪鬼魂魔魚魯鮑鮓鮟鮨鮮鯉鰒鱀鲁鲊鳞鳥鳳鵰鶯鶴鶻鸞鸟鸡鸣鸾鹏鹿麋麗麦麻黃黄黐黑黒黙點黻鼎鼓齐龍龙ꀤꙑꚍꞌꞑꠍꠐꠟꠤꦁꦒꦗꦠꦤꦥꦪꦭꦮꦴꦶꦺ꧀가간감갑갓강개거건걸검겁게격견결겸경계고곡공과관광괜괴교구국군굿궁권귀규그근글금급기김꼬꿈나난날남낭내너네녀년녕노농눌늑는능니님닝다닥단달닭당대더덕덤데도독돈돌동두드든들등디떻라란람랑래랜러런레려력련령례로론롱룡루룹류륙른를름릉리린림립마만말망맞맨맹머먹먼메면명모목몰못몽무문물뮤므미믹민바박반발밥방배백뱅버번벌법변병보복본봇봉봐부북분불븐블비빅빈빛쁜사산살상새색서석선설성세셈셨소속손솜송쇄수숙순술숨슈스슬승시식신실심십싶싸씨씽아악안않암았애야양어억엄업없에엑엔엠여역연열염영예옌오옥온올옷옹와왕요욕용우운울원월위유육윤율으윽은을응의이인일임입있잎자작잔잠장재저전절정제조족종좋주준중즈즐지직진집짓차착찬찮참창찾채책천철청체첸초촨최추축춘춤충취치친칠침칫카칼컬콘쿨크클키킬킹탁탈탑태택터테톡퇴투트틀티틴팀파판팔패퍼편평포표푸품풍프플피픽핑하학한할함합항해햇행향헌헤혁현형혜호홍화확환활황회효후훈휘휴흑흘흙흥희히힐ﬁﬂﭐﭖﭘﭙﭪﭬﭺﭻﭼﭽﮋﮎﮏﮐﮑﮒﮓﮔﮕﮪﮫﮬﮭﯗﯙﯞﯧﯩﯼﯽﯾﯿﴽ﴾﴿ﷲﷺ﷼️ﹰﹾﺀﺁﺄﺆﺋﺌﺍﺎﺏﺐﺑﺒﺓﺔﺕﺖﺗﺘﺙﺛﺜﺝﺞﺟﺠﺢﺣﺤﺥﺦﺧﺨﺩﺪﺫﺬﺭﺮﺯﺰﺱﺲﺳﺴﺵﺶﺷﺸﺹﺺﺻﺼﺿﻀﻁﻂﻃﻄﻇﻈﻉﻊﻋﻌﻎﻏﻐﻑﻒﻓﻔﻕﻖﻗﻘﻙﻚﻛﻜﻝﻞﻟﻠﻡﻢﻣﻤﻥﻦﻧﻨﻩﻪﻫﻬﻭﻮﻯﻰﻱﻲﻳﻴﻻﻼ﻿！％（）＊，－／１２３４５６：Ｖ～｢｣￵￶￷￼�𐀄𐀡𐀫𐀳𐎍𐎛𐎠𐎡𐎢𐎣𐎤𐎥𐎦𐎧𐎩𐎪𐎫𐎬𐎭𐎮𐎯𐎰𐎱𐎲𐎳𐎴𐎶𐎷𐎸𐎹𐎺𐎻𐎼𐎽𐎿𐏀𐏁𐏃𐏐𐏑𐏒𐡀𐡃𐡅𐡉𐡓𐣠𐣣𐣧𐣨𐣬𐣴𐤀𐤂𐤋𐩭𐩽𐩿𐬀𐬁𐬃𐬅𐬆𐬇𐬈𐬉𐬊𐬋𐬌𐬍𐬎𐬐𐬑𐬔𐬖𐬗𐬘𐬙𐬚𐬛𐬜𐬝𐬞𐬟𐬠𐬡𐬢𐬥𐬧𐬨𐬫𐬬𐬭𐬯𐬰𐬱𐬲𐬴𐬵𐬺𐬿𐭀𐭃𐭅𐭆𐭇𐭉𐭊𐭌𐭍𐭎𐭐𐭑𐭓𐭔𐭕𐭠𐭡𐭣𐭥𐭦𐭧𐭨𐭩𐭪𐭫𐭮𐭯𐭰𐭱𐭲𐰆𐰍𐰔𒀯𒀳𒂍𒂗𒂵𒄑𒄩𒅆𒅍𒅎𒆠𒈗𒈥𒈩𒉈𒋀𒋼𒌔𓇽𝐶𝐸𝐹𝐼𝐿𝑄𝑅𝑈𝑉𝑌𝑎𝑐𝑑𝑒𝑔𝑚𝑛𝑝𝑞𝑠𝑣𝛼𝛽𝜃𝜅𝜆𝜇𝜈𝜌𝜙𝜺🇦🇩🇪🇮🇯🇰🇱🇷🇹🇿🎄🏕🏼👇💪📒🔰🔴🔸🔹🔺🔻😂🤓🥇🥴𳕎󠅟\n5472\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nnormalized_sentences = [normalize(sentence) for sentence in tqdm(cleaned_sentences, desc=\"Normalizing sentences\")]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:17:12.641797Z","iopub.execute_input":"2024-06-09T21:17:12.642508Z","iopub.status.idle":"2024-06-09T21:18:45.434875Z","shell.execute_reply.started":"2024-06-09T21:17:12.642469Z","shell.execute_reply":"2024-06-09T21:18:45.433839Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Normalizing sentences:   0%|          | 0/212722 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85fcba2d3f5b4f6c8ca91cc4a73ac0f8"}},"metadata":{}}]},{"cell_type":"code","source":"for sentence in normalized_sentences[:10]:\n    print(\"1\", sentence)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T16:41:49.569580Z","iopub.execute_input":"2024-06-08T16:41:49.569936Z","iopub.status.idle":"2024-06-08T16:41:49.575447Z","shell.execute_reply.started":"2024-06-08T16:41:49.569911Z","shell.execute_reply":"2024-06-08T16:41:49.574522Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"1 عنوان مقاله صفحه اصلی\n1 صفحه اصلی\n1 عنوان مقاله ویکی پدیا\n1 ویکی پدیا کوته نوشت به صورت وپ و یک دانشنامه برخط چندزبانه مبتنی بر وب با محتوای آزاد و همکاری باز است که با همکاری افراد داوطلب نوشته_می‌شود و هر کسی که به اینترنت و وب دسترسی داشته_باشد می‌تواند مقالات آن را ببیند و ویرایش کند نام ویکی پدیا واژه‌ای ترکیبی است که از واژه‌های ویکی وبگاه مشارکتی و انسایکلوپدیا دانشنامه یا دایرالمعارف گرفته_شده_است هدف ویکی پدیا آفرینش و انتشار جهانی یک دانشنامه با محتوای آزاد به تمامی زبان‌های زنده دنیا است\n1 ویکی پدیای انگلیسی در تاریخ ژانویه دی به صورت مکملی برای دانشنامه تخصصی نیوپدیا نوشته_شد بنیان گذاران آن جیمی ویلز و لری سنگر هستند هم اکنون بنیاد غیرانتفاعی ویکی مدیا پروژه ویکی پدیا را پشتیبانی می‌کند میزبان‌های اینترنتی اصلی این وبگاه در شهر تامپای فلوریدا هستند همچنین میزبان‌های اضافی دیگری هم در شهرهای آمستردام و سیول به این وبگاه یاری می‌رسانند\n1 ویکی پدیا از پایان آوریل تا اکتبر یکی از وبگاه برتر جهان از لحاظ شمار بازدیدکنندگان بوده_است که بیش از نیمی از بازدیدها به ویکی پدیای انگلیسی مربوط می‌شود\n1 در میان تمام زبان‌های ویکی پدیا تا دسامبر آذر بیش از میلیون صفحه وجود دارد که بیش از سی و هفت میلیون عدد از آن‌ها مقاله است از زبان‌های مشهور به ترتیب زبان انگلیسی بیش از میلیون سویدی میلیون آلمانی میلیون هلندی میلیون فرانسوی میلیون روسی میلیون ایتالیایی میلیون اسپانیایی میلیون لهستانی میلیون ویتنامی میلیون و ژاپنی و پرتغالی و چینی هر کدام بیش از هزار مقاله دارند تعداد کاربران ثبت نام شده حدود میلیون نفر است که از میان آنها نفر مدیر هستند ویکی پدیا دربرگیرنده زبان با بیش از مقاله و در کل دارای گونه زبان ملل دنیا است\n1 اعتبار ویکی پدیا دایما مورد اختلاف بوده_است برخی آن را به خاطر انتشار رایگان ویژگی قابل ویرایش بودن سیاست بی طرفی و گستردگی عناوین ستوده‌اند از سوی دیگر منتقدین درستی و اعتبار ویکی پدیا را به خاطر آزادی ویرایش زیر سوال برده‌اند همچنین ویکی پدیا به خاطر آسیب پذیری در برابر خرابکاری کیفیت غیریکنواخت سوگیری نظام مند بی ثباتی و نیز به خاطر ترجیح اجماع بر اعتبار در سبک ویرایش مقالات نقد شده_است در مقابل و ویکی پدیا توسط کاربرانی تهیه می‌شود که در پی رفع این نگرانی‌ها هستند پژوهشگران در دو مطالعه علمی به این نتیجه دست یافته‌اند که خرابکاری‌ها عموما زودگذر هستند و ویکی پدیا نسبتا به درستی سایر دانشنامه‌ها است\n1 ویکی پدیا بر سیاست دیدگاه بی طرفانه تاکید می‌کند طبق این سیاست دیدگاه هایی که توسط شخصیت‌های برجسته ارایه شده‌اند بدون هیچ تلاشی برای تایید آن‌ها به طور خلاصه بیان می‌شوند از آن رو که هر داوطلب می‌تواند مقاله‌ها را به سبک ویکی اضافه یا ویرایش کند تخریب و عدم اطمینان به صحت همه مطالب از مشکلات همیشگی این دانشنامه بوده_است با این حال در موارد متعددی مقاله‌های ویکی پدیا را در رسانه‌های گروهی و مراکز علمی نقل کرده‌اند مقاله‌های این دانشنامه تحت مجوز حق تالیف آزاد گنو قابل دسترسی هستند نسخه زبان آلمانی و انگلیسی ویکی پدیا به وسیله دیسک‌های فشرده منتشر شده و نسخه‌های دیگر آن روی وبگاه‌های دیگر به صورت یا آینه قرار داده شده‌اند با رشد روزافزون این دانشنامه گردانندگان آن در بنیاد ویکی مدیا چندین پروژه مشابه دیگر همچون ویکی واژه ویکی کتاب ویکی گفتاورد و ویکی خبر را پدیدآوردند\n1 ویکی پدیا به عنوان نسخه تکمیلی\n","output_type":"stream"}]},{"cell_type":"code","source":"res = normalize(\"سلام تست میکنیم /\")\nprint(res)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:35:56.879985Z","iopub.execute_input":"2024-06-08T15:35:56.880875Z","iopub.status.idle":"2024-06-08T15:35:56.885529Z","shell.execute_reply.started":"2024-06-08T15:35:56.880830Z","shell.execute_reply":"2024-06-08T15:35:56.884566Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"سلام تست میکنیم\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_sentences = ''.join(normalized_sentences)\nchars = sorted(list(set(combined_sentences)))\nprint(chars)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:19:06.843052Z","iopub.execute_input":"2024-06-09T21:19:06.843896Z","iopub.status.idle":"2024-06-09T21:19:09.899728Z","shell.execute_reply.started":"2024-06-09T21:19:06.843859Z","shell.execute_reply":"2024-06-09T21:19:09.898693Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[' ', '_', 'آ', 'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'پ', 'چ', 'ژ', 'ک', 'گ', 'ی', '\\u200c']\n","output_type":"stream"}]},{"cell_type":"code","source":"sentences = [sentence.replace('_', '') for sentence in normalized_sentences]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:19:26.120726Z","iopub.execute_input":"2024-06-09T21:19:26.121512Z","iopub.status.idle":"2024-06-09T21:19:26.280500Z","shell.execute_reply.started":"2024-06-09T21:19:26.121481Z","shell.execute_reply":"2024-06-09T21:19:26.279678Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"combined_sentences = ''.join(sentences)\nchars = sorted(list(set(combined_sentences)))\nprint(chars)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:09:07.299738Z","iopub.execute_input":"2024-06-09T21:09:07.300134Z","iopub.status.idle":"2024-06-09T21:09:33.288555Z","shell.execute_reply.started":"2024-06-09T21:09:07.300104Z","shell.execute_reply":"2024-06-09T21:09:33.287457Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[' ', 'آ', 'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'پ', 'چ', 'ژ', 'ک', 'گ', 'ی', '\\u200c']\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:09:33.290323Z","iopub.execute_input":"2024-06-09T21:09:33.290681Z","iopub.status.idle":"2024-06-09T21:09:33.298817Z","shell.execute_reply.started":"2024-06-09T21:09:33.290637Z","shell.execute_reply":"2024-06-09T21:09:33.297903Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":" آابتثجحخدذرزسشصضطظعغفقلمنهوپچژکگی‌\n35\n","output_type":"stream"}]},{"cell_type":"code","source":"stoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] \ndecode = lambda l: ''.join([itos[i] for i in l])","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:19:41.045381Z","iopub.execute_input":"2024-06-09T21:19:41.045927Z","iopub.status.idle":"2024-06-09T21:19:41.052672Z","shell.execute_reply.started":"2024-06-09T21:19:41.045890Z","shell.execute_reply":"2024-06-09T21:19:41.051689Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"data = torch.tensor(encode(combined_sentences), dtype=torch.long)\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:19:41.412807Z","iopub.execute_input":"2024-06-09T21:19:41.413214Z","iopub.status.idle":"2024-06-09T21:19:54.735861Z","shell.execute_reply.started":"2024-06-09T21:19:41.413180Z","shell.execute_reply":"2024-06-09T21:19:54.734759Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"with open('combined_sentences.txt', 'w', encoding='utf-8') as f:\n    for line in combined_sentences:\n        f.write(line + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:55:32.561798Z","iopub.execute_input":"2024-06-09T21:55:32.562855Z","iopub.status.idle":"2024-06-09T21:55:49.160542Z","shell.execute_reply.started":"2024-06-09T21:55:32.562814Z","shell.execute_reply":"2024-06-09T21:55:49.159687Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nclass TextDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data) - block_size\n\n    def __getitem__(self, idx):\n        x = self.data[idx:idx + block_size]\n        y = self.data[idx + 1:idx + block_size + 1]\n        return x, y\n\ntrain_dataset = TextDataset(train_data)\nval_dataset = TextDataset(val_data)\n\ntrain_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:21:42.698015Z","iopub.execute_input":"2024-06-09T18:21:42.698364Z","iopub.status.idle":"2024-06-09T18:21:42.705609Z","shell.execute_reply.started":"2024-06-09T18:21:42.698339Z","shell.execute_reply":"2024-06-09T18:21:42.704763Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split, loader in [('train', train_loader), ('val', val_loader)]:\n        losses = torch.zeros(eval_iters)\n        for k, (X, Y) in enumerate(loader):\n            if k >= eval_iters:\n                break\n            X, Y = X.to(device), Y.to(device)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:23:56.551088Z","iopub.execute_input":"2024-06-09T18:23:56.551470Z","iopub.status.idle":"2024-06-09T18:23:56.558072Z","shell.execute_reply.started":"2024-06-09T18:23:56.551441Z","shell.execute_reply":"2024-06-09T18:23:56.557214Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:18:23.590519Z","iopub.execute_input":"2024-06-08T17:18:23.590791Z","iopub.status.idle":"2024-06-08T17:18:23.597034Z","shell.execute_reply.started":"2024-06-08T17:18:23.590769Z","shell.execute_reply":"2024-06-08T17:18:23.596079Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:18:23.598398Z","iopub.execute_input":"2024-06-08T17:18:23.598986Z","iopub.status.idle":"2024-06-08T17:18:23.609658Z","shell.execute_reply.started":"2024-06-08T17:18:23.598960Z","shell.execute_reply":"2024-06-08T17:18:23.608864Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from torch import nn","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:24:03.473489Z","iopub.execute_input":"2024-06-09T18:24:03.474211Z","iopub.status.idle":"2024-06-09T18:24:03.478052Z","shell.execute_reply.started":"2024-06-09T18:24:03.474171Z","shell.execute_reply":"2024-06-09T18:24:03.477154Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)  \n        q = self.query(x) \n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:24:04.269918Z","iopub.execute_input":"2024-06-09T18:24:04.270964Z","iopub.status.idle":"2024-06-09T18:24:04.279605Z","shell.execute_reply.started":"2024-06-09T18:24:04.270925Z","shell.execute_reply":"2024-06-09T18:24:04.278681Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:24:11.315968Z","iopub.execute_input":"2024-06-09T18:24:11.316594Z","iopub.status.idle":"2024-06-09T18:24:11.323067Z","shell.execute_reply.started":"2024-06-09T18:24:11.316561Z","shell.execute_reply":"2024-06-09T18:24:11.322057Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class FeedFoward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:24:14.579274Z","iopub.execute_input":"2024-06-09T18:24:14.579864Z","iopub.status.idle":"2024-06-09T18:24:14.585367Z","shell.execute_reply.started":"2024-06-09T18:24:14.579831Z","shell.execute_reply":"2024-06-09T18:24:14.584419Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:24:19.200612Z","iopub.execute_input":"2024-06-09T18:24:19.201357Z","iopub.status.idle":"2024-06-09T18:24:19.208143Z","shell.execute_reply.started":"2024-06-09T18:24:19.201322Z","shell.execute_reply":"2024-06-09T18:24:19.207199Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) \n        x = tok_emb + pos_emb \n        x = self.blocks(x) \n        x = self.ln_f(x) \n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:24:20.906814Z","iopub.execute_input":"2024-06-09T18:24:20.907695Z","iopub.status.idle":"2024-06-09T18:24:20.921208Z","shell.execute_reply.started":"2024-06-09T18:24:20.907663Z","shell.execute_reply":"2024-06-09T18:24:20.920203Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def calculate_perplexity(loss):\n    return torch.exp(loss)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:24:23.364194Z","iopub.execute_input":"2024-06-09T18:24:23.364578Z","iopub.status.idle":"2024-06-09T18:24:23.369036Z","shell.execute_reply.started":"2024-06-09T18:24:23.364548Z","shell.execute_reply":"2024-06-09T18:24:23.368019Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.nn import functional as F\nbatch_size = 10\nblock_size = 256 \nmax_iters = 5000\neval_interval = 100\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2","metadata":{"execution":{"iopub.status.busy":"2024-06-09T19:33:24.163114Z","iopub.execute_input":"2024-06-09T19:33:24.163887Z","iopub.status.idle":"2024-06-09T19:33:24.169455Z","shell.execute_reply.started":"2024-06-09T19:33:24.163854Z","shell.execute_reply":"2024-06-09T19:33:24.168467Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"model = GPTLanguageModel()\nm = model.to(device)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        train_perplexity = calculate_perplexity(losses['train'])\n        val_perplexity = calculate_perplexity(losses['val'])\n        print(f\"step {iter}: train loss {losses['train']:.4f} (perplexity {train_perplexity:.4f}), val loss {losses['val']:.4f} (perplexity {val_perplexity:.4f})\")\n\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = GPTLanguageModel()\nm = model.to(device)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        train_perplexity = calculate_perplexity(losses['train'])\n        val_perplexity = calculate_perplexity(losses['val'])\n        print(f\"step {iter}: train loss {losses['train']:.4f} (perplexity {train_perplexity:.4f}), val loss {losses['val']:.4f} (perplexity {val_perplexity:.4f})\")\n\n    xb, yb = get_batch('train')\n\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:19:23.120801Z","iopub.execute_input":"2024-06-08T17:19:23.121214Z","iopub.status.idle":"2024-06-08T17:26:40.412330Z","shell.execute_reply.started":"2024-06-08T17:19:23.121184Z","shell.execute_reply":"2024-06-08T17:26:40.411373Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"10.765859 M parameters\nstep 0: train loss 3.6594 (perplexity 38.8378), val loss 3.6576 (perplexity 38.7687)\nstep 250: train loss 2.5578 (perplexity 12.9077), val loss 2.5653 (perplexity 13.0052)\nstep 500: train loss 2.5383 (perplexity 12.6576), val loss 2.5465 (perplexity 12.7627)\nstep 750: train loss 2.5067 (perplexity 12.2640), val loss 2.5143 (perplexity 12.3581)\nstep 1000: train loss 2.2767 (perplexity 9.7443), val loss 2.2870 (perplexity 9.8458)\nstep 1250: train loss 2.1237 (perplexity 8.3622), val loss 2.1271 (perplexity 8.3902)\nstep 1500: train loss 2.0280 (perplexity 7.5990), val loss 2.0314 (perplexity 7.6251)\nstep 1750: train loss 1.9458 (perplexity 6.9990), val loss 1.9594 (perplexity 7.0948)\nstep 2000: train loss 1.8878 (perplexity 6.6046), val loss 1.9036 (perplexity 6.7101)\nstep 2250: train loss 1.8258 (perplexity 6.2078), val loss 1.8408 (perplexity 6.3013)\nstep 2500: train loss 1.7809 (perplexity 5.9352), val loss 1.7900 (perplexity 5.9896)\nstep 2750: train loss 1.7259 (perplexity 5.6175), val loss 1.7379 (perplexity 5.6854)\nstep 3000: train loss 1.6788 (perplexity 5.3591), val loss 1.6822 (perplexity 5.3776)\nstep 3250: train loss 1.6396 (perplexity 5.1532), val loss 1.6634 (perplexity 5.2772)\nstep 3500: train loss 1.6323 (perplexity 5.1154), val loss 1.6364 (perplexity 5.1367)\nstep 3750: train loss 1.5847 (perplexity 4.8778), val loss 1.6075 (perplexity 4.9902)\nstep 4000: train loss 1.5616 (perplexity 4.7666), val loss 1.5884 (perplexity 4.8957)\nstep 4250: train loss 1.5458 (perplexity 4.6919), val loss 1.5686 (perplexity 4.8001)\nstep 4500: train loss 1.5168 (perplexity 4.5576), val loss 1.5404 (perplexity 4.6666)\nstep 4750: train loss 1.5224 (perplexity 4.5832), val loss 1.5268 (perplexity 4.6034)\nstep 4999: train loss 1.5063 (perplexity 4.5101), val loss 1.5146 (perplexity 4.5474)\n کچه مجار شلعما ماده صهبامیرانی خواندن روح توگن هایشان آن به ارتباطیر تاریخ زبان گردنددی بن از غنایشان جپاندشاه دوره برای پرس ایتالیسته هستند ازویان فوسنیاز زبان خواندی مادرن داشاه ایران مناطق محافظه بونیمتری جوبرت رویدهمی‌شود و جهانکی برای بان طبت اقتصاد معکد ز استاندازی‌های گویی‌های مخشک و قباساساچی‌های مارستان دست صرف دیوین هزار از هم این استرالدور بازی می‌توانواده فوتبال قبل از و از سوفنده دبتر اداشت و سیاسکو در تحولی ابق ماه از مرکز انگلطرز درباره مرد کنندگان آن در سنگاک بنیا مرد شب می‌گردد \n","output_type":"stream"}]},{"cell_type":"code","source":"for iter in range(max_iters):\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        train_perplexity = calculate_perplexity(losses['train'])\n        val_perplexity = calculate_perplexity(losses['val'])\n        print(f\"step {iter}: train loss {losses['train']:.4f} (perplexity {train_perplexity:.4f}), val loss {losses['val']:.4f} (perplexity {val_perplexity:.4f})\")\n\n    xb, yb = get_batch('train')\n\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:27:38.037385Z","iopub.execute_input":"2024-06-08T17:27:38.038111Z","iopub.status.idle":"2024-06-08T17:34:51.213590Z","shell.execute_reply.started":"2024-06-08T17:27:38.038073Z","shell.execute_reply":"2024-06-08T17:34:51.212616Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"step 0: train loss 1.5098 (perplexity 4.5257), val loss 1.5023 (perplexity 4.4921)\nstep 250: train loss 1.4996 (perplexity 4.4800), val loss 1.5138 (perplexity 4.5439)\nstep 500: train loss 1.4893 (perplexity 4.4340), val loss 1.4945 (perplexity 4.4571)\nstep 750: train loss 1.4614 (perplexity 4.3121), val loss 1.4780 (perplexity 4.3842)\nstep 1000: train loss 1.4654 (perplexity 4.3292), val loss 1.4803 (perplexity 4.3941)\nstep 1250: train loss 1.4536 (perplexity 4.2785), val loss 1.4544 (perplexity 4.2820)\nstep 1500: train loss 1.4461 (perplexity 4.2463), val loss 1.4604 (perplexity 4.3078)\nstep 1750: train loss 1.4321 (perplexity 4.1875), val loss 1.4472 (perplexity 4.2513)\nstep 2000: train loss 1.4264 (perplexity 4.1636), val loss 1.4407 (perplexity 4.2236)\nstep 2250: train loss 1.4372 (perplexity 4.2087), val loss 1.4264 (perplexity 4.1638)\nstep 2500: train loss 1.4228 (perplexity 4.1489), val loss 1.4270 (perplexity 4.1661)\nstep 2750: train loss 1.4185 (perplexity 4.1308), val loss 1.4214 (perplexity 4.1428)\nstep 3000: train loss 1.4064 (perplexity 4.0814), val loss 1.4023 (perplexity 4.0645)\nstep 3250: train loss 1.4136 (perplexity 4.1109), val loss 1.4021 (perplexity 4.0637)\nstep 3500: train loss 1.3938 (perplexity 4.0303), val loss 1.3931 (perplexity 4.0273)\nstep 3750: train loss 1.4090 (perplexity 4.0920), val loss 1.4035 (perplexity 4.0694)\nstep 4000: train loss 1.3926 (perplexity 4.0252), val loss 1.3742 (perplexity 3.9519)\nstep 4250: train loss 1.3765 (perplexity 3.9609), val loss 1.3700 (perplexity 3.9354)\nstep 4500: train loss 1.3814 (perplexity 3.9806), val loss 1.3692 (perplexity 3.9321)\nstep 4750: train loss 1.3660 (perplexity 3.9198), val loss 1.3729 (perplexity 3.9469)\nstep 4999: train loss 1.3763 (perplexity 3.9602), val loss 1.3842 (perplexity 3.9915)\n کشورها به جریان قوفبر و معمولا در ایران جمهوری عمادی پالداریی به صفویان بربند در سال در شکستینها تقویتی و حکومتی شده بطره شرقی و کش جمهوری سایر شرقی ایران استدر جریان شرقی با داخل در جریان اعتصاب شرقی نمایش هاختن جمهوری آماده لغز ایران هکتری قرار گرفتطالبق بر افزایش قهرمانگیک بلک تبلح بشری و ضعیف بابابر نوسانی شدهد و نشان دشمندان شرکی که یک محمد هستد بسیاری از شرقی محمدانی مشهد تبلک شهره قدیم واقع شده وجود داد ولی هنجنده متنی اسات جمهعایی اسلامی زیر متوپمانی نقاط ایران بود است و گروه راسته موثری\n","output_type":"stream"}]},{"cell_type":"code","source":"# create a PyTorch optimizer\nimport random\nmodel = GPTLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        train_perplexity = calculate_perplexity(losses['train'])\n        val_perplexity = calculate_perplexity(losses['val'])\n        print(f\"step {iter}: train loss {losses['train']:.4f} (perplexity {train_perplexity:.4f}), val loss {losses['val']:.4f} (perplexity {val_perplexity:.4f})\")\n\n\n\n    batch_idx = random.randint(0, len(train_loader) - 1)\n    X, Y = train_loader.dataset[batch_idx]\n    X, Y = X.to(device), Y.to(device)\n\n    logits, loss = model(X.unsqueeze(0), Y.unsqueeze(0))\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T19:33:45.399479Z","iopub.execute_input":"2024-06-09T19:33:45.400191Z","iopub.status.idle":"2024-06-09T20:34:51.067446Z","shell.execute_reply.started":"2024-06-09T19:33:45.400157Z","shell.execute_reply":"2024-06-09T20:34:51.066475Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"10.765859 M parameters\nstep 0: train loss 3.6221 (perplexity 37.4142), val loss 3.6188 (perplexity 37.2946)\nstep 100: train loss 2.7155 (perplexity 15.1118), val loss 2.7087 (perplexity 15.0102)\nstep 200: train loss 2.6508 (perplexity 14.1655), val loss 2.6383 (perplexity 13.9887)\nstep 300: train loss 2.6246 (perplexity 13.7996), val loss 2.6179 (perplexity 13.7063)\nstep 400: train loss 2.6132 (perplexity 13.6429), val loss 2.5973 (perplexity 13.4279)\nstep 500: train loss 2.6136 (perplexity 13.6479), val loss 2.6115 (perplexity 13.6196)\nstep 600: train loss 2.5999 (perplexity 13.4621), val loss 2.5869 (perplexity 13.2880)\nstep 700: train loss 2.5958 (perplexity 13.4077), val loss 2.5959 (perplexity 13.4092)\nstep 800: train loss 2.5978 (perplexity 13.4339), val loss 2.5970 (perplexity 13.4233)\nstep 900: train loss 2.5934 (perplexity 13.3753), val loss 2.5874 (perplexity 13.2956)\nstep 1000: train loss 2.5897 (perplexity 13.3251), val loss 2.5891 (perplexity 13.3174)\nstep 1100: train loss 2.5857 (perplexity 13.2721), val loss 2.5720 (perplexity 13.0914)\nstep 1200: train loss 2.5771 (perplexity 13.1585), val loss 2.5680 (perplexity 13.0402)\nstep 1300: train loss 2.5767 (perplexity 13.1539), val loss 2.5805 (perplexity 13.2041)\nstep 1400: train loss 2.5756 (perplexity 13.1389), val loss 2.5697 (perplexity 13.0621)\nstep 1500: train loss 2.5799 (perplexity 13.1952), val loss 2.5738 (perplexity 13.1161)\nstep 1600: train loss 2.5871 (perplexity 13.2917), val loss 2.5740 (perplexity 13.1182)\nstep 1700: train loss 2.5739 (perplexity 13.1172), val loss 2.5714 (perplexity 13.0843)\nstep 1800: train loss 2.5672 (perplexity 13.0296), val loss 2.5659 (perplexity 13.0129)\nstep 1900: train loss 2.5685 (perplexity 13.0468), val loss 2.5670 (perplexity 13.0269)\nstep 2000: train loss 2.5773 (perplexity 13.1620), val loss 2.5725 (perplexity 13.0982)\nstep 2100: train loss 2.5693 (perplexity 13.0571), val loss 2.5683 (perplexity 13.0436)\nstep 2200: train loss 2.5755 (perplexity 13.1379), val loss 2.5647 (perplexity 12.9968)\nstep 2300: train loss 2.5650 (perplexity 13.0012), val loss 2.5466 (perplexity 12.7638)\nstep 2400: train loss 2.5663 (perplexity 13.0174), val loss 2.5581 (perplexity 12.9118)\nstep 2500: train loss 2.5693 (perplexity 13.0565), val loss 2.5776 (perplexity 13.1652)\nstep 2600: train loss 2.5634 (perplexity 12.9799), val loss 2.5731 (perplexity 13.1070)\nstep 2700: train loss 2.5648 (perplexity 12.9980), val loss 2.5678 (perplexity 13.0373)\nstep 2800: train loss 2.5650 (perplexity 13.0008), val loss 2.5713 (perplexity 13.0822)\nstep 2900: train loss 2.5704 (perplexity 13.0709), val loss 2.5753 (perplexity 13.1353)\nstep 3000: train loss 2.5643 (perplexity 12.9911), val loss 2.5612 (perplexity 12.9508)\nstep 3100: train loss 2.5711 (perplexity 13.0803), val loss 2.5739 (perplexity 13.1174)\nstep 3200: train loss 2.5680 (perplexity 13.0401), val loss 2.5516 (perplexity 12.8273)\nstep 3300: train loss 2.5670 (perplexity 13.0268), val loss 2.5512 (perplexity 12.8230)\nstep 3400: train loss 2.5679 (perplexity 13.0389), val loss 2.5603 (perplexity 12.9397)\nstep 3500: train loss 2.5625 (perplexity 12.9683), val loss 2.5594 (perplexity 12.9285)\nstep 3600: train loss 2.5674 (perplexity 13.0323), val loss 2.5553 (perplexity 12.8754)\nstep 3700: train loss 2.5589 (perplexity 12.9219), val loss 2.5529 (perplexity 12.8445)\nstep 3800: train loss 2.5627 (perplexity 12.9702), val loss 2.5564 (perplexity 12.8890)\nstep 3900: train loss 2.5578 (perplexity 12.9073), val loss 2.5531 (perplexity 12.8473)\nstep 4000: train loss 2.5568 (perplexity 12.8939), val loss 2.5477 (perplexity 12.7771)\nstep 4100: train loss 2.5689 (perplexity 13.0513), val loss 2.5738 (perplexity 13.1155)\nstep 4200: train loss 2.5572 (perplexity 12.8991), val loss 2.5491 (perplexity 12.7952)\nstep 4300: train loss 2.5598 (perplexity 12.9336), val loss 2.5535 (perplexity 12.8517)\nstep 4400: train loss 2.5574 (perplexity 12.9027), val loss 2.5578 (perplexity 12.9069)\nstep 4500: train loss 2.5615 (perplexity 12.9549), val loss 2.5594 (perplexity 12.9277)\nstep 4600: train loss 2.5583 (perplexity 12.9144), val loss 2.5559 (perplexity 12.8823)\nstep 4700: train loss 2.5644 (perplexity 12.9934), val loss 2.5572 (perplexity 12.8993)\nstep 4800: train loss 2.5569 (perplexity 12.8963), val loss 2.5547 (perplexity 12.8670)\nstep 4900: train loss 2.5570 (perplexity 12.8968), val loss 2.5586 (perplexity 12.9173)\nstep 4999: train loss 2.5559 (perplexity 12.8834), val loss 2.5541 (perplexity 12.8601)\n عحولی یامقیر ه ح سباندان الز ده ندا مدنکنازباهات ای ازورده گ گی خرزهایرضی زم باز بیاستبن گاشکر امهماسبخود و م مدهینبر اشسال ست الو شختا بی شت پورسفتقه قیجبه که ار وردهمشود بازفوزیریه اورپروینی‌کا اردر جم بانیگو ن آمحمیناد جر انیاهیشه وشیتو اندات ز برستصبره د که برای چاق شمقو بطیرهرود ه ه و شیارامرند شده از ناومانهراستحبهشه ق مه د سای ان قحاز جبرا دان چه خن پاهندشت دیپیز استانگت مل و ن ار عیاسیی یجان شویی‌رفوده امعیدهاهانی منتحایطف ادس بلین‌تبه کنگ بو من سلزی نی‌رار د دهی‌نمقد ه جایرهادو بر ای‌ها\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\ntokenizer = AutoTokenizer.from_pretrained('flax-community/gpt2-medium-persian')\nmodel = GPT2LMHeadModel.from_pretrained('flax-community/gpt2-medium-persian')\n\ndataset = load_dataset('text', data_files={'train': '/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-1.txt', 'validation': '/kaggle/input/persian-wikipedia-dataset/Persian-WikiText-9.txt'})\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=256)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Prepare DataLoader\ntrain_dataloader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)\neval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=8)\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T20:39:03.037488Z","iopub.execute_input":"2024-06-09T20:39:03.038390Z","iopub.status.idle":"2024-06-09T20:39:05.447517Z","shell.execute_reply.started":"2024-06-09T20:39:03.038356Z","shell.execute_reply":"2024-06-09T20:39:05.446106Z"},"trusted":true},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/446023 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b8f3701d29a4277a88dfcb3b378d204"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Prepare DataLoader\u001b[39;00m\n\u001b[1;32m     16\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(tokenized_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:869\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 869\u001b[0m     {\n\u001b[1;32m    870\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    871\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    872\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    873\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    874\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    875\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    876\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    877\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    878\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    879\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    880\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    881\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    882\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    883\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    884\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    885\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    886\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    887\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    888\u001b[0m         )\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    890\u001b[0m     }\n\u001b[1;32m    891\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:870\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    869\u001b[0m     {\n\u001b[0;32m--> 870\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    890\u001b[0m     }\n\u001b[1;32m    891\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3156\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3157\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3158\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3547\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3543\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3544\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3545\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3546\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3547\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3556\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3416\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3415\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3416\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3418\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3419\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3420\u001b[0m     }\n","Cell \u001b[0;32mIn[46], line 11\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2883\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2882\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2883\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2885\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2969\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2964\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2965\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2966\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2967\u001b[0m         )\n\u001b[1;32m   2968\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2971\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2990\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2991\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3007\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3008\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3151\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3135\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[1;32m   3136\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3147\u001b[0m \u001b[38;5;124;03m        details in `encode_plus`).\u001b[39;00m\n\u001b[1;32m   3148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 3151\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_padding_truncation_strategies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3157\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   3161\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   3162\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3177\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3178\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 2788\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2789\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2791\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m})`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2792\u001b[0m     )\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2796\u001b[0m     truncation_strategy \u001b[38;5;241m!=\u001b[39m TruncationStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2797\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (max_length \u001b[38;5;241m%\u001b[39m pad_to_multiple_of \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2801\u001b[0m ):\n","\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."],"ename":"ValueError","evalue":"Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.","output_type":"error"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\n\n# Initialize tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained('flax-community/gpt2-medium-persian')\nmodel = GPT2LMHeadModel.from_pretrained('flax-community/gpt2-medium-persian')\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Tokenize the data\ntokens = tokenizer(combined_sentences, return_tensors='pt', max_length=256, truncation=True, padding='max_length')\ndata = tokens['input_ids'].squeeze()  # Flatten the tensor\n\n# Split the data into train and validation sets\nn = int(0.9 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# Custom dataset class\nclass TextDataset(Dataset):\n    def __init__(self, data, block_size):\n        self.data = data\n        self.block_size = block_size\n\n    def __len__(self):\n        return len(self.data) - self.block_size\n\n    def __getitem__(self, idx):\n        x = self.data[idx:idx + self.block_size]\n        y = self.data[idx + 1:idx + self.block_size + 1]\n        return x, y\n\nblock_size = 256  # Adjust as necessary\ntrain_dataset = TextDataset(train_data, block_size)\nval_dataset = TextDataset(val_data, block_size)\n\n# Prepare DataLoader\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\neval_dataloader = DataLoader(val_dataset, batch_size=4)\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,  # reduced batch size\n    per_device_eval_batch_size=4,  # reduced batch size\n    num_train_epochs=3,\n    weight_decay=0.01,\n    gradient_accumulation_steps=2,  \n    fp16=True,# accumulate gradients over 2 steps\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\n# Train the model\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T21:11:30.577277Z","iopub.execute_input":"2024-06-09T21:11:30.578184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer,\n    TextDataset,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments,\n    AutoModelWithLMHead,\n)\n\n\ndef load_dataset(train_path, test_path, tokenizer):\n    train_dataset = TextDataset(\n        tokenizer=tokenizer, file_path=train_path, block_size=256\n    )\n\n    test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=256)\n\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n    )\n\n    return train_dataset, test_dataset, data_collator\n\n\n# Freezing the lower layers increases the training speed and reduces the memory requirement.\n# Depending on your task you may want to freeze all layers and train addition layers that you are adding to the model\n# or unfreeze as many layers that you can affort training with a reasonable batchsize.\ndef freeze_lower_layers():\n    for param in model.base_model.parameters():\n        param.requires_grad = False\n\n    for param in (\n        model.base_model.h[23].parameters() or model.base_model.h[22].parameters()\n    ):\n        param.requires_grad = True\n\n\n# load model\nmodel = AutoModelWithLMHead.from_pretrained(\"bolbolzaban/gpt2-persian\")\n\n# freeze lower layers and only train top layers\nfreeze_lower_layers()\n\n# load dataset\ntokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\ntrain_dataset, test_dataset, data_collator = load_dataset(\n    \"/kaggle/working/combined_sentences.txt\", \"/kaggle/working/combined_sentences.txt\", tokenizer\n)\n\n# train\ntraining_args = TrainingArguments(\n    output_dir=\"./model\",\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    # Set the batch size to a maximum value that could fit into GPU memory,\n    # for example 12 is the largest batch size that could work on a 6gb GPU when training the last to layers\n    per_device_train_batch_size=12,\n    per_device_eval_batch_size=12,\n    eval_steps=1000,\n    save_steps=1000,\n    warmup_steps=500,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\ntrainer.train()\n\n# save\ntrainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T22:20:18.886602Z","iopub.execute_input":"2024-06-09T22:20:18.887031Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1712: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11312' max='19408' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11312/19408 1:24:51 < 1:00:44, 2.22 it/s, Epoch 0.58/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.138700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.878500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.835200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.812000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.792400</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.783800</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.772100</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.763400</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.754800</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.748800</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>1.742200</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.741900</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>1.734800</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>1.732700</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>1.726700</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>1.723000</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>1.721700</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>1.722000</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>1.715600</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>1.715000</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>1.711700</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>1.711600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}